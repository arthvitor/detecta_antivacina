{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto - Detector de _tweets_ Antivacina - Tratamento e Treinamento do Aprendizado de M√°quina\n",
    "Detector de tweets antivacina | Esse projeto tem como objetivo raspar tweets recentes da API oficial do Twitter, fazer uma an√°lise explorat√≥ria das m√©tricas e treinar e classificar automaticamente os tweets como potenciais pr√≥-vacina ou antivacina. \n",
    "\n",
    "Projeto realizado como trabalho final da disciplina Pensamento Computacional, do programa de p√≥s-gradua√ß√£o em Jornalismo de Dados, Automa√ß√£o e Data Storytelling do Insper\n",
    "\n",
    "Profs: Guilherme Dias Felitti e Alvaro Jusen\n",
    "\n",
    "OBS: Requisi√ß√µes realizadas localmente por problemas de RAM do Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vitor\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Instalando bibliotecas\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vitor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vitor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# importando bibliotecas de processamento de linguagem natural\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier, SklearnClassifier  # algoritmo de machile learning\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# fazendo download dos pr√©-requisitos para fun√ß√µes do nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 3 - Tratando o banco de dados e criando um arquivo de treino para o aprendizado de m√°quina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergindo as duas fases da coleta\n",
    "pd.set_option('display.max_column', 500)\n",
    "df1 = pd.read_csv(r'dataset1.csv')\n",
    "df2 = pd.read_csv(r'dataset2.csv')\n",
    "df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergindo as duas fases da coleta\n",
    "pd.set_option('display.max_column', 500)\n",
    "df1 = pd.read_csv(r'dataset1.csv')\n",
    "df2 = pd.read_csv(r'dataset2.csv')\n",
    "df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retirando linhas duplicadas na coleta\n",
    "df.drop_duplicates(subset='idleft', inplace=True)\n",
    "\n",
    "# criando dataframe que ser√° utilizado nessa an√°lise\n",
    "tweets = df[['text', 'source', 'created_at', 'lang', 'author_id', 'public_metrics.retweet_count', 'public_metrics.reply_count', \\\n",
    "    'public_metrics.like_count', 'public_metrics.quote_count', 'type']]\n",
    "tweets.rename(columns={\n",
    "    'text': 'tweet',\n",
    "    'source': 'fonte',\n",
    "    'created_at': 'data_postagem',\n",
    "    'lang': 'idioma',\n",
    "    'author_id': 'id_autor',\n",
    "    'public_metrics.retweet_count': 'retweet',\n",
    "    'public_metrics.reply_count': 'respostas',\n",
    "    'public_metrics.like_count': 'likes',\n",
    "    'public_metrics.quote_count': 'men√ß√µes',\n",
    "    'type': 'tipo'\n",
    "}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tratando as datas \n",
    "formato = '%Y-%m-%d'\n",
    "tweets['data_postagem'] = pd.to_datetime(tweets['data_postagem']).dt.to_period('D')\n",
    "\n",
    "# retirando qualquer tweet que n√£o seja em portugu√™s\n",
    "tweets = tweets.query(' idioma == \"pt\"  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando amostra para fazer o treinando para machine learning \n",
    "tweets_amostra = tweets[['tweet','tipo']]\n",
    "tweets_amostra = tweets_amostra.query(' tipo != \"retweeted\" ')  # pegando tweets autorais\n",
    "tweets_amostra = tweets_amostra.sample(300)\n",
    "tweets_amostra.to_csv('corpus_treino2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 4: Treinando a m√°quina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicando par√¢metros de an√°lise**\n",
    "\n",
    "_Pr√≥-vacina_: Qualquer tweet que apresente comportamento de defesa e endo√ßamento para vacina√ß√£o ou informa√ß√µes sobre onde se vacina\n",
    "\n",
    "_Antivacina_: Qualquer tweet que apresente comportamento da inefic√°cia, perigos ou de defesa da tese que vacina √© t√≥xica\n",
    "\n",
    "_Neutro_: Qualquer tweet que apresente comportamento sem nenhum juizo de valor sobre a vacina ou que apresente quest√µes que n√£o seja inerentes sobre a efic√°cia da vacina ou que questione sua exist√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recebendo arquivos de treino\n",
    "neutro = r'C:\\Users\\Vitor\\Creative Cloud Files\\Insper\\2¬∫ Trimestre\\Pensamento Computacional\\projeto_antivacina\\corpus_neu.data'\n",
    "pro_vacina = r'C:\\Users\\Vitor\\Creative Cloud Files\\Insper\\2¬∫ Trimestre\\Pensamento Computacional\\projeto_antivacina\\corpus_pro.data'\n",
    "anti_vacina = r'C:\\Users\\Vitor\\Creative Cloud Files\\Insper\\2¬∫ Trimestre\\Pensamento Computacional\\projeto_antivacina\\corpus_anti.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para fazer tokeniza√ß√£o das frases\n",
    "def tokenizar(entrada):\n",
    "    lista_token = list()\n",
    "    for palavra in entrada:\n",
    "        token = word_tokenize(palavra)\n",
    "        lista_token.append(token)\n",
    "    return lista_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retirando os stopwords\n",
    "def tirar_stop(entrada):\n",
    "\n",
    "  # criando vari√°veis\n",
    "  lista_palavras = list()\n",
    "  final = list()\n",
    "\n",
    "  # listando as stopwords\n",
    "  stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "  outras_stop = ['!', ':', '#', 'https', ',', '?', '.', '-', '(', ')', '@', 't.co', '...', '``', \"''\", \"‚û°Ô∏è\", '1¬∫', '2¬∫', '3¬∫', '4¬∫', '5¬∫', '6¬∫', 'üîóhttps', ]\n",
    "  for stop in outras_stop:\n",
    "    stopwords.append(stop)\n",
    "\n",
    "  # retirando as stopwords\n",
    "  for palavra in entrada:\n",
    "    if palavra not in stopwords:\n",
    "      palavra.replace('.co','.')\n",
    "      palavra.replace('//t.co','.')\n",
    "      lista_palavras.append(palavra)\n",
    "  \n",
    "  # retirando parte dos links\n",
    "  for i in lista_palavras:\n",
    "    p = i.replace('.co','.')\n",
    "    p = i.replace('//t.co','.')\n",
    "    final.append(p)\n",
    "  \n",
    "  return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de Bag of words\n",
    "def bag(entrada):\n",
    "  bag_of_words = dict()\n",
    "\n",
    "  # criando features\n",
    "  for p in entrada:\n",
    "    bag_of_words[p] = entrada.count(p)\n",
    "  return bag_of_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o algoritimo de Aprendizado de M√°quina Supervisionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando a m√°quina\n",
    "def treinamento(pro_vacina, anti_vacina, neutro):\n",
    "\n",
    "  # criando listas\n",
    "  pro_dados = list()\n",
    "  ant_dados = list()\n",
    "  neu_dados = list()\n",
    "\n",
    "  # alocando os arquivos de treino\n",
    "  # positivo\n",
    "  with open(pro_vacina, 'r', encoding='utf8') as arquivo:\n",
    "    linhas = arquivo.readlines()\n",
    "    for linha in linhas:\n",
    "      if len(linha) > 0:\n",
    "        pro_dados.append(linha.lower())\n",
    "  \n",
    "  # negativo\n",
    "  with open(anti_vacina, 'r', encoding='utf8') as arquivo:\n",
    "    linhas = arquivo.readlines()\n",
    "    for linha in linhas:\n",
    "      if len(linha) > 0:\n",
    "        ant_dados.append(linha.lower()) \n",
    "\n",
    "  # neutro\n",
    "  with open(neutro, 'r', encoding='utf8') as arquivo:\n",
    "    linhas = arquivo.readlines()\n",
    "    for linha in linhas:\n",
    "      if len(linha) > 0:\n",
    "        neu_dados.append(linha.lower())\n",
    "\n",
    "  # tokenizando palavras, retirando stopwords e retirando as bags of words\n",
    "  profeats = list()\n",
    "  for l in tokenizar(pro_dados):\n",
    "    sem_stop = tirar_stop(l)\n",
    "    profeats.append((bag(sem_stop), 'pro_vacina'))\n",
    "  print(profeats)\n",
    "\n",
    "  antifeats = list()\n",
    "  for l in tokenizar(ant_dados):\n",
    "    sem_stop = tirar_stop(l)\n",
    "    antifeats.append((bag(sem_stop), 'antivacina'))\n",
    "  print(antifeats)\n",
    "\n",
    "  neufeats = list()\n",
    "  for l in tokenizar(neu_dados):\n",
    "    sem_stop = tirar_stop(l)\n",
    "    neufeats.append((bag(sem_stop), 'neutro'))\n",
    "  print(neufeats)\n",
    "\n",
    "  # criando string de treino\n",
    "  treino = profeats + antifeats + neufeats\n",
    "\n",
    "  # Rodando os algoritmos para treina-los\n",
    "  classificadorME = MaxentClassifier.train(treino, 'GIS', trace=0, \n",
    "                                           encoding=None, labels=None, \n",
    "                                           gaussian_prior_sigma=0, max_iter = 1)\n",
    "  \n",
    "  #SVM\n",
    "  classificadorSVM = SklearnClassifier(LinearSVC(), sparse=False)\n",
    "  classificadorSVM.train(treino)\n",
    "\n",
    "  # Naive Bayes\n",
    "  classificadorNB = NaiveBayesClassifier.train(treino)\n",
    "\n",
    "  return ([classificadorME, classificadorSVM, classificadorNB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando algoritmo\n",
    "classificadores = treinamento(pro_vacina, anti_vacina, neutro)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 5: Aplicando algoritmo de aprendizado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando um Dataframe para fazer an√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copiando um dataframe para receber a classifica√ß√£o\n",
    "df_classificacao = tweets\n",
    "\n",
    "# tokenizando as palavras\n",
    "df_classificacao['bag_of_words'] = tokenizar(df_classificacao['tweet'])\n",
    "\n",
    "# Retirando Stopwords\n",
    "limpo = list()\n",
    "for l in df_classificacao['bag_of_words']:\n",
    "  ss = tirar_stop(l)\n",
    "  limpo.append(ss)\n",
    "df_classificacao['sem_stop'] = limpo\n",
    "df_classificacao['bag_of_words'] = limpo\n",
    "\n",
    "# Criando bag of words\n",
    "dfbag_of_words = list()\n",
    "for l in df_classificacao['bag_of_words']:\n",
    "  bow = bag(l)\n",
    "  dfbag_of_words.append(bow)\n",
    "df_classificacao['bag_of_words'] = dfbag_of_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando modelo que foi treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando fun√ß√£o para classificar tweets\n",
    "def classificar(entrada, nome, coluna_dataframe, classificador):\n",
    "  classificacao = list()\n",
    "  for l in entrada[coluna_dataframe]:\n",
    "    classificado = classificadores[classificador].classify(l)\n",
    "    classificacao.append(classificado)\n",
    "  entrada[nome] = classificacao\n",
    "\n",
    "\n",
    "## MAIN\n",
    "classificar(df_classificacao, 'classificacao_MaxentClassifier', 'bag_of_words', 0)\n",
    "classificar(df_classificacao, 'classificacao_SklearnClassifier', 'bag_of_words', 1)\n",
    "classificar(df_classificacao, 'classificacao_NaiveBayesClassifier', 'bag_of_words', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportando resultados\n",
    "df_classificacao.to_csv('tweets_classificados.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a23c1fff748fd79ca478c64f9bf3d1c6a1797515f98b9b89ff038832e665379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
